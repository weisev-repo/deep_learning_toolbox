{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple graph example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# simple computational graph - numpy\n",
    "N, D = 3, 4\n",
    "np.random.seed(0)\n",
    "x = np.random.randn(N, D)\n",
    "y = np.random.randn(N, D)\n",
    "z = np.random.randn(N, D)\n",
    "\n",
    "a = x * y  # shape (N, D)\n",
    "b = a + z  # shape (N, D)\n",
    "c = np.sum(b)\n",
    "\n",
    "grad_c = 1.0\n",
    "grad_b = grad_c * np.ones((N,D))\n",
    "grad_a = grad_b.copy()\n",
    "grad_z = grad_b.copy()\n",
    "grad_x = grad_a * y\n",
    "grad_y = grad_a * x\n",
    "\n",
    "print(c)\n",
    "print(grad_x)\n",
    "print(grad_y)\n",
    "print(grad_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TENSORFLOW 2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# simple computational graph with tensorflow\n",
    "\n",
    "@tf.function # this decorator tells tf that a graph is defined\n",
    "def simple_graph(x, y, z):\n",
    "    a = x * y\n",
    "    b = a + z\n",
    "    c = tf.reduce_sum(input_tensor=b)\n",
    "    grad_x, grad_y, grad_z = tf.gradients(ys=c, xs=[x, y, z])\n",
    "    return c, grad_x, grad_y, grad_z\n",
    "\n",
    "N, D = 3, 4\n",
    "np.random.seed(0)\n",
    "x = np.random.randn(N, D)\n",
    "y = np.random.randn(N, D)\n",
    "z = np.random.randn(N, D)\n",
    "\n",
    "c_val, grad_x_val, grad_y_val, grad_z_val = simple_graph(x, y, z)\n",
    "\n",
    "print('c_val =', c_val)\n",
    "print('grad_x_val =', grad_x_val)\n",
    "print('grad_y_val =', grad_y_val)\n",
    "print('grad_z_val =', grad_z_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "print(\"Num CPUs Available: \", len(tf.config.experimental.list_physical_devices('CPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just as example to show how it looks in PyTorch \n",
    "# You can skip this cell if you don't want to install PyTorch\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# simple computational graph - torch\n",
    "N, D = 3, 4\n",
    "\n",
    "x = Variable(torch.randn(N, D), \n",
    "             requires_grad=True)\n",
    "y = Variable(torch.randn(N, D), \n",
    "             requires_grad=True)\n",
    "z = Variable(torch.randn(N, D), \n",
    "             requires_grad=True)\n",
    "\n",
    "a = x * y\n",
    "b = a + z\n",
    "c = torch.sum(b)\n",
    "\n",
    "c.backward()\n",
    "\n",
    "print(x.grad.data)\n",
    "print(y.grad.data)\n",
    "print(z.grad.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A more complex example of graph\n",
    "\n",
    "### Utility function - plot history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_history(epoch_vector, losses_list, name_list, y_label):\n",
    "    # plt.figure(figsize=(12,8))\n",
    "    for i, losses in enumerate(losses_list):\n",
    "        plt.plot(epoch_vector, losses, label=name_list[i])\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel(y_label)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0: get some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First get MNIST data and prepare it\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train_vec),(x_test, y_test_vec) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "x_train = x_train.reshape(60000, 784)\n",
    "x_test = x_test.reshape(10000, 784)\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = tf.keras.utils.to_categorical(y_train_vec, 10, dtype='float64')\n",
    "y_test = tf.keras.utils.to_categorical(y_test_vec, 10, dtype='float64')\n",
    "N = x_train.shape[0]         # number of samples\n",
    "D = x_train.shape[1]         # dimension of input sample\n",
    "n_classes = y_train.shape[1] # output dim\n",
    "print('MNIST data set ready. N={}, D={}, n_classes={}'.format(N,D,n_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1\n",
    "This graph is actually creating the necessary nodes to:\n",
    "1. Do the forward pass (line 8)\n",
    "2. Compute the MSE loss (lines 9 and 10)\n",
    "3. Compute the gradients of the loss w.r.t. the weights (line 11)\n",
    "\n",
    "In line 20, we init the weights (outside of the graph, using numpy). In line 23 we use the graph, passing the whole train set (inputs and targets) and getting out of the graph the predicted outputs, the loss and the gradient values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow 2.0 - a more complex example\n",
    "# step 1 - forward pass on single layer ANN with ReLu\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "@tf.function # this decorator tells tf that a graph is defined\n",
    "def simple_ann_train(x, w1, y):\n",
    "    y_pred = tf.maximum(tf.matmul(x, w1), 0) # ReLU on logit\n",
    "    diff = y_pred - y\n",
    "    loss = tf.reduce_mean(tf.pow(diff, 2))\n",
    "    grad = tf.gradients(ys=loss, xs=[w1]) \n",
    "    # tf.gradients returns a list of sum(dy/dx) for each x in xs\n",
    "    return y_pred, loss, grad\n",
    "\n",
    "N = x_train.shape[0]         # number of samples\n",
    "D = x_train.shape[1]         # dimension of input sample\n",
    "n_classes = y_train.shape[1] # output dim\n",
    "\n",
    "np.random.seed(0)\n",
    "w1 = np.random.randn(D, n_classes)\n",
    "\n",
    "with tf.device('/CPU:0'):  # change to /GPU:0 to move to GPU\n",
    "    out = simple_ann_train(x_train, w1, y_train)\n",
    "\n",
    "y_pred, loss_val, grad = out\n",
    "grad_w1 = grad[0] #grad is a list\n",
    "print(loss_val)\n",
    "print(grad_w1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow 2.0 - a more complex example\n",
    "# step 2 - training but weights are passing from gpu to cpu\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "@tf.function # this decorator tells tf that a graph is defined\n",
    "def simple_ann_train(x, w1, y):\n",
    "    y_pred = tf.maximum(tf.matmul(x, w1), 0) # ReLU on logit\n",
    "    diff = y_pred - y\n",
    "    loss = tf.reduce_mean(tf.pow(diff, 2))\n",
    "    grad = tf.gradients(ys=loss, xs=[w1])\n",
    "    # tf.gradients returns a list of sum(dy/dx) for each x in xs\n",
    "    return y_pred, loss, grad\n",
    "\n",
    "np.random.seed(0)\n",
    "w1 = np.random.randn(D, n_classes)\n",
    "alpha = 1e-2\n",
    "J = []\n",
    "for epoch in range(40):\n",
    "    with tf.device('/CPU:0'):  # change to /GPU:0 to move it to GPU\n",
    "        out = simple_ann_train(x_train, w1, y_train)\n",
    "    y_pred, loss_val, grad = out\n",
    "    grad_w1 = grad[0] # grad is a list of gradients\n",
    "    w1 -= alpha * grad_w1.numpy()\n",
    "    J.append(loss_val)\n",
    "    print(\"epoch = {}, loss = {}\".format(epoch,loss_val))\n",
    "\n",
    "plt.plot(J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow 2.0 - a more complex example\n",
    "# step 3 - change weights to persistant in-graph variables\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "@tf.function # this decorator tells tf that a graph is defined\n",
    "def simple_ann_train(x, y, alpha):\n",
    "    y_pred = tf.maximum(tf.matmul(x, w1), 0) # ReLU on logit\n",
    "    diff = y_pred - y\n",
    "    loss = tf.reduce_mean(tf.pow(diff, 2))\n",
    "    grad = tf.gradients(ys=loss, xs=[w1])\n",
    "    # tf.gradients returns a list of sum(dy/dx) for each x in xs\n",
    "    grad_w1 = grad[0]\n",
    "    w1.assign(w1 - alpha * grad_w1)\n",
    "    return y_pred, loss\n",
    "\n",
    "np.random.seed(0)\n",
    "alpha = 1e-2\n",
    "J = []\n",
    "w1 = tf.Variable(tf.random.normal((D, n_classes), dtype='float64'))\n",
    "for epoch in range(40):\n",
    "    with tf.device('/CPU:0'):  # change to /GPU:0 to move it to GPU\n",
    "        out = simple_ann_train(x_train, y_train, alpha)\n",
    "    y_pred, loss_val = out\n",
    "    J.append(loss_val)\n",
    "    print(\"epoch = {}, loss = {}\".format(epoch,loss_val))\n",
    "\n",
    "plt.plot(J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow 2.0 - a more complex example\n",
    "# step 4 - use pre-defined losses and optimizers\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "@tf.function # this decorator tells tf that a graph is defined\n",
    "def simple_ann_train(x, y, alpha):\n",
    "    y_pred = tf.nn.relu(tf.matmul(x, w1))  # ReLU on logit\n",
    "    mse = tf.keras.losses.MeanSquaredError()\n",
    "    loss = mse(y, y_pred)\n",
    "    optimizer = tf.compat.v1.train.GradientDescentOptimizer(1e-2)\n",
    "    updates = optimizer.minimize(loss, var_list=w1)\n",
    "    return y_pred, loss\n",
    "\n",
    "np.random.seed(0)\n",
    "alpha = 1e-2\n",
    "J = []\n",
    "w1 = tf.Variable(tf.random.normal((D, n_classes), dtype='float64'))\n",
    "for epoch in range(40):\n",
    "    with tf.device('/CPU:0'):  # change to /GPU:0 to move it to GPU\n",
    "        out = simple_ann_train(x_train, y_train, alpha)\n",
    "    y_pred, loss_val = out\n",
    "    J.append(loss_val)\n",
    "    print(\"epoch = {}, loss = {}\".format(epoch,loss_val))\n",
    "\n",
    "plt.plot(J)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using pre-defined models with Keras "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Keras\n",
    "# Single layer network architecture, similar to the one used in the previous steps.\n",
    "# Full batch SGD\n",
    "import tensorflow as tf\n",
    "\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Dense(10, input_shape=(D,), use_bias=False, activation='relu'))\n",
    "model.summary()\n",
    "\n",
    "sgd = tf.keras.optimizers.SGD(learning_rate=0.5)\n",
    "\n",
    "model.compile(optimizer=sgd, loss='mse', metrics=['accuracy'])\n",
    "\n",
    "history1 = model.fit(x_train, y_train, batch_size=N, epochs=40)\n",
    "\n",
    "model.evaluate(x_test,  y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Keras\n",
    "# Single layer network architecture, similar to the one used in the previous steps.\n",
    "# Batch=128 SGD\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Dense(10, input_shape=(D,), use_bias=False, activation='relu'))\n",
    "model.summary()\n",
    "\n",
    "sgd = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "\n",
    "model.compile(optimizer=sgd, loss='mse', metrics=['accuracy'])\n",
    "\n",
    "history2 = model.fit(x_train, y_train, batch_size=128, epochs=40)\n",
    "\n",
    "model.evaluate(x_test,  y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history1.epoch, [history1.history['loss'], history2.history['loss']], \n",
    "             ['full batch', 'batch=128'], 'mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
