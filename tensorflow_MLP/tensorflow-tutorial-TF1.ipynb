{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple graph example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# simple computational graph - numpy\n",
    "N, D = 3, 4\n",
    "np.random.seed(0)\n",
    "x = np.random.randn(N, D)\n",
    "y = np.random.randn(N, D)\n",
    "z = np.random.randn(N, D)\n",
    "\n",
    "a = x * y  # shape (N, D)\n",
    "b = a + z  # shape (N, D)\n",
    "c = np.sum(b)\n",
    "\n",
    "grad_c = 1.0\n",
    "grad_b = grad_c * np.ones((N,D))\n",
    "grad_a = grad_b.copy()\n",
    "grad_z = grad_b.copy()\n",
    "grad_x = grad_a * y\n",
    "grad_y = grad_a * x\n",
    "\n",
    "print(c)\n",
    "print(grad_x)\n",
    "print(grad_y)\n",
    "print(grad_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TENSORFLOW 1.X\n",
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "\n",
    "tf.disable_v2_behavior() # to use former syntax of\n",
    "                         # tensorflow 1.X, eg 1.14\n",
    "\n",
    "# simple computational graph with tensorflow \n",
    "N, D = 3, 4\n",
    "np.random.seed(0)\n",
    "x = tf.placeholder(tf.float32, shape=(N, D))\n",
    "y = tf.placeholder(tf.float32, shape=(N, D))\n",
    "z = tf.placeholder(tf.float32, shape=(N, D))\n",
    "\n",
    "a = x * y\n",
    "b = a + z\n",
    "c = tf.reduce_sum(b)\n",
    "\n",
    "grad_x, grad_y, grad_z = tf.gradients(c, [x, y, z])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    values = {\n",
    "        x: np.random.randn(N, D),\n",
    "        y: np.random.randn(N, D),\n",
    "        z: np.random.randn(N, D)\n",
    "    }\n",
    "    out = sess.run([c, grad_x, grad_y, grad_z], \n",
    "                   feed_dict=values)\n",
    "    c_val, grad_x_val, grad_y_val, grad_z_val = out\n",
    "\n",
    "    print(c_val)\n",
    "    print(grad_x_val)\n",
    "    print(grad_y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "print(\"Num CPUs Available: \", len(tf.config.experimental.list_physical_devices('CPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "\n",
    "tf.disable_v2_behavior() # to use former syntax of\n",
    "                         # tensorflow 1.X, eg 1.14\n",
    "\n",
    "# simple computational graph - tensorflow on gpu\n",
    "N, D = 3, 4\n",
    "with tf.device('/CPU:0'): # '/GPU:0' for cpu exec \n",
    "    x = tf.placeholder(tf.float32, shape=(N, D))\n",
    "    y = tf.placeholder(tf.float32, shape=(N, D))\n",
    "    z = tf.placeholder(tf.float32, shape=(N, D))\n",
    "    a = x * y\n",
    "    b = a + z\n",
    "    c = tf.reduce_sum(b)\n",
    "\n",
    "    grad_x, grad_y, grad_z = tf.gradients(c, [x, y, z])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    values = {\n",
    "        x: np.random.randn(N, D),\n",
    "        y: np.random.randn(N, D),\n",
    "        z: np.random.randn(N, D)\n",
    "    }\n",
    "    out = sess.run([c, grad_x, grad_y, grad_z],\n",
    "                   feed_dict=values)\n",
    "    c_val, grad_x_val, grad_y_val, grad_z_val = out\n",
    "\n",
    "    print(c_val)\n",
    "    print(grad_x_val)\n",
    "    print(grad_y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just as example to show how it looks in PyTorch \n",
    "# You can skip this cell if you don't want to install PyTorch\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# simple computational graph - torch on gpu\n",
    "N, D = 3, 4\n",
    "\n",
    "x = Variable(torch.randn(N, D), \n",
    "             requires_grad=True)\n",
    "y = Variable(torch.randn(N, D), \n",
    "             requires_grad=True)\n",
    "z = Variable(torch.randn(N, D), \n",
    "             requires_grad=True)\n",
    "\n",
    "a = x * y\n",
    "b = a + z\n",
    "c = torch.sum(b)\n",
    "\n",
    "c.backward()\n",
    "\n",
    "print(x.grad.data)\n",
    "print(y.grad.data)\n",
    "print(z.grad.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A more complex example of graph\n",
    "\n",
    "### Utility function - plot history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_history(epoch_vector, losses_list, name_list, y_label):\n",
    "    # plt.figure(figsize=(12,8))\n",
    "    for i, losses in enumerate(losses_list):\n",
    "        plt.plot(epoch_vector, losses, label=name_list[i])\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel(y_label)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0: get some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First get some data and prepare it\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train_vec),(x_test, y_test_vec) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "x_train = x_train.reshape(60000, 784)\n",
    "x_test = x_test.reshape(10000, 784)\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = tf.keras.utils.to_categorical(y_train_vec, 10, dtype='float64')\n",
    "y_test = tf.keras.utils.to_categorical(y_test_vec, 10, dtype='float64')\n",
    "N = x_train.shape[0]  # number of samples\n",
    "D = x_train.shape[1]  # dimension of input sample\n",
    "n_classes = y_train.shape[1] # output dim\n",
    "print('MNIST data set ready. N={}, D={}, n_classes={}'.format(N,D,n_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1\n",
    "This graph is actually creating the necessary nodes to:\n",
    "1. Do the forward pass (line 13)\n",
    "2. Compute the MSE loss (lines 14 and 15)\n",
    "3. Compute the gradients of the loss w.r.t. the weights (line 17)\n",
    "\n",
    "In line 23, we init the weights (outside of the graph, using numpy). In line 25 we use the graph, passing the whole train set in the feed_dict (inputs, targets, weight) and getting out of the graph the loss and the gradient values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow 1.x - a more complex example\n",
    "# step 1 - forward pass on single layer ANN with ReLu\n",
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "\n",
    "tf.disable_v2_behavior() # to use former syntax of\n",
    "                         # tensorflow 1.X, eg 1.14\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=(N, D))\n",
    "y = tf.placeholder(tf.float32, shape=(N, n_classes))\n",
    "w1 = tf.placeholder(tf.float32, shape=(D, n_classes))\n",
    "\n",
    "y_pred = tf.maximum(tf.matmul(x, w1), 0) # ReLU on logit\n",
    "diff = y_pred - y\n",
    "loss = tf.reduce_mean(tf.pow(diff, 2))\n",
    "\n",
    "grad_w1 = tf.gradients(loss, [w1])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    values = {\n",
    "        x: x_train,\n",
    "        y: y_train,\n",
    "        w1: np.random.randn(D, n_classes)\n",
    "    }\n",
    "    out = sess.run([loss, grad_w1], feed_dict=values)\n",
    "    loss_val, grad_w1_val = out\n",
    "\n",
    "print(loss_val, np.array(grad_w1_val).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow 1.x - a more complex example\n",
    "# step 2 - training but weights are passing from gpu to cpu\n",
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "from matplotlib import pyplot as pl\n",
    "\n",
    "tf.disable_v2_behavior() # to use former syntax of\n",
    "                         # tensorflow 1.X, eg 1.14\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=(N, D))\n",
    "y = tf.placeholder(tf.float32, shape=(N, n_classes))\n",
    "w1 = tf.placeholder(tf.float32, shape=(D, n_classes))\n",
    "\n",
    "y_pred = tf.maximum(tf.matmul(x, w1), 0) # ReLU on logit\n",
    "diff = y_pred - y\n",
    "loss = tf.reduce_mean(tf.pow(diff, 2))\n",
    "\n",
    "grad_w1 = tf.gradients(loss, [w1])[0] # returns a list of\n",
    "                                      # sums of gradients\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    values = {\n",
    "        x: x_train,\n",
    "        y: y_train,\n",
    "        w1: np.random.randn(D, n_classes)\n",
    "    }\n",
    "    alpha = 1e-2\n",
    "    J = []\n",
    "    for epoch in range(20):\n",
    "        out = sess.run([loss, grad_w1], feed_dict=values)\n",
    "        loss_val, grad_w1_val = out\n",
    "        values[w1] -= alpha * grad_w1_val\n",
    "        J.append(loss_val)\n",
    "        print(\"epoch\", epoch, loss_val)\n",
    "    pl.plot(J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow 1.x - a more complex example\n",
    "# step 3 - change weights from placehoders to variables\n",
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "from matplotlib import pyplot as pl\n",
    "\n",
    "tf.disable_v2_behavior() # to use former syntax of\n",
    "                         # tensorflow 1.X, eg 1.14\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=(N, D))\n",
    "y = tf.placeholder(tf.float32, shape=(N, n_classes))\n",
    "w1 = tf.Variable(tf.random_normal((D, n_classes)))\n",
    "\n",
    "y_pred = tf.maximum(tf.matmul(x, w1), 0.0) # ReLU on logit\n",
    "diff = y_pred - y\n",
    "loss = tf.reduce_mean(tf.pow(diff, 2))\n",
    "grad_w1 = tf.gradients(loss, [w1])[0] # returns a list of\n",
    "                                      # sums of gradients\n",
    "\n",
    "alpha = 1e-2\n",
    "new_w1 = w1.assign(w1 - alpha * grad_w1)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    values = { x: x_train, y: y_train}\n",
    "    J = []\n",
    "    for epoch in range(20):\n",
    "        loss_val = sess.run([loss], feed_dict=values)\n",
    "        J.append(loss_val)\n",
    "        print(\"epoch\", epoch, loss_val)\n",
    "    pl.plot(J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow 1.x - a more complex example\n",
    "# step 3bis - add dummy graph node that depends on updates\n",
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "from matplotlib import pyplot as pl\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=(N, D))\n",
    "y = tf.placeholder(tf.float32, shape=(N, n_classes))\n",
    "w1 = tf.Variable(tf.random_normal((D, n_classes)))\n",
    "\n",
    "y_pred = tf.maximum(tf.matmul(x, w1), 0.0) # ReLU on logit\n",
    "diff = y_pred - y\n",
    "loss = tf.reduce_mean(tf.pow(diff, 2))\n",
    "grad_w1 = tf.gradients(loss, [w1])[0] # returns a list of\n",
    "                                      # sums of gradients\n",
    "\n",
    "alpha = 1e-2\n",
    "new_w1 = w1.assign(w1 - alpha * grad_w1)\n",
    "updates = tf.group(new_w1)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    values = { x: x_train, y: y_train}\n",
    "    J = []\n",
    "    for epoch in range(20):\n",
    "        loss_val = sess.run([loss, updates], feed_dict=values)\n",
    "        J.append(loss_val)\n",
    "        print(\"epoch\", epoch, loss_val)\n",
    "    pl.plot(J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow 1.x - a more complex example\n",
    "# step 4 - use optimizer and pre-defined loss\n",
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "from matplotlib import pyplot as pl\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=(N, D))\n",
    "y = tf.placeholder(tf.float32, shape=(N, n_classes))\n",
    "w1 = tf.Variable(tf.random_normal((D, n_classes)))\n",
    "\n",
    "y_pred = tf.maximum(tf.matmul(x, w1), 0.0) # ReLU on logit\n",
    "diff = y_pred - y\n",
    "loss = tf.losses.mean_squared_error(y_pred, y)\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(1e-2)\n",
    "updates = optimizer.minimize(loss)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    values = { x: x_train, y: y_train}\n",
    "    J = []\n",
    "    for epoch in range(20):\n",
    "        loss_val = sess.run([loss, updates], feed_dict=values)\n",
    "        J.append(loss_val)\n",
    "        print(\"epoch\", epoch, loss_val)\n",
    "    pl.plot(J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow 1.x - a more complex example\n",
    "# step 5 - use layers and initilizer\n",
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "from matplotlib import pyplot as pl\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=(N, D))\n",
    "y = tf.placeholder(tf.float32, shape=(N, n_classes))\n",
    "\n",
    "init = tf.variance_scaling_initializer(2.0) # He init\n",
    "y_pred = tf.layers.dense(inputs=x, units=n_classes, \n",
    "                         activation=tf.nn.relu, \n",
    "                         kernel_initializer=init)\n",
    "\n",
    "loss = tf.losses.mean_squared_error(y_pred, y)\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(1e-2)\n",
    "updates = optimizer.minimize(loss)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    values = { x: x_train, y: y_train}\n",
    "    J = []\n",
    "    for epoch in range(20):\n",
    "        loss_val = sess.run([loss, updates], \n",
    "                            feed_dict=values)\n",
    "        J.append(loss_val)\n",
    "        print(\"epoch\", epoch, loss_val[0])\n",
    "    pl.plot(J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
