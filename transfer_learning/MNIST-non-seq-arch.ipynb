{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments with Keras functional API on MNIST\n",
    "\n",
    "This notebook will guide you through the use of the `keras` functional API. You are going to use the `mnist` dataset from LeCun et al. 1998\n",
    "\n",
    "We assume you are using TF 2. If you need to install some packages, use `pip install ...`, e.g. `pip install sklearn` for SciKit Learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, import TF and get its version.\n",
    "import tensorflow as tf\n",
    "tf_version = tf.__version__\n",
    "\n",
    "# Check if version >=2.0.0 is used\n",
    "if not tf_version.startswith('2.'):\n",
    "    print('WARNING: TensorFlow >= 2.0.0 will be used in this course.\\nYour version is {}'.format(tf_version) + '.\\033[0m')\n",
    "else:\n",
    "    print('OK: TensorFlow >= 2.0.0' + '.\\033[0m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as pl\n",
    "\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import utils\n",
    "from sklearn import metrics as me"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the raw data\n",
    "First load the `mnist` dataset and normalize it to be in the range [0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "X_train = X_train.reshape(60000, 784)\n",
    "X_test = X_test.reshape(10000, 784)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255.0\n",
    "X_test /= 255.0\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target values of the network are supposed to be 1-hot targets. Now the `y_train` is an array with scalar values as in `[5 0 4 1 ...]` and it should be a 1-hot array `Y_train` as in : \n",
    "\n",
    "`[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
    " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
    " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
    " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]...]`\n",
    " \n",
    "Note the change of capital letter in the `Y_train` to denote, per convention, an array with multiple dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 10\n",
    "Y_train = utils.to_categorical(y_train, n_classes)\n",
    "Y_test = utils.to_categorical(y_test, n_classes)\n",
    "print(Y_train[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP\n",
    "Here is an example of Multi-Layer Perceptron first defined with the Sequential model then with the functional model. We will also need to tell Keras what is the size of our inputs, in our case a linearized vector of size D=784."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import plot_model\n",
    "H = 300               # number of neurons\n",
    "D = X_train.shape[1]  # dimension of input - 784 for MNIST\n",
    "\n",
    "#Keras sequential model\n",
    "model1 = Sequential()\n",
    "model1.add(Dense(H, input_shape=(D,), activation='relu'))\n",
    "model1.add(Dense(n_classes, activation='softmax'))\n",
    "model1.summary()\n",
    "\n",
    "#Keras functional API\n",
    "visible = Input(shape=(D,)) # func api, input is declared\n",
    "hidden1 = Dense(H, activation='relu')(visible)\n",
    "output = Dense(n_classes, activation='softmax')(hidden1)\n",
    "model2 = Model(inputs=visible, outputs=output)\n",
    "model2.summary()\n",
    "plot_model(model2, to_file='multilayer_perceptron_graph.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = 128\n",
    "E = 10\n",
    "model2.compile(loss='categorical_crossentropy', optimizer='rmsprop', \n",
    "              metrics=['accuracy'])\n",
    "log = model2.fit(X_train, Y_train, batch_size=B, epochs=E,\n",
    "                    verbose=1, validation_data=(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.plot(log.history['loss'], label='Training')\n",
    "pl.plot(log.history['val_loss'], label='Testing')\n",
    "pl.legend()\n",
    "pl.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_test, metric_test = model2.evaluate(X_test, Y_test)\n",
    "print('Test loss:', loss_test)\n",
    "print('Test accuracy:', metric_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional neural network - CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-shape the data\n",
    "X_train = X_train.reshape(X_train.shape[0], 28, 28, 1).astype('float32')\n",
    "X_test = X_test.reshape(X_test.shape[0], 28, 28, 1).astype('float32')\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN - Keras functional API\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "\n",
    "visible = Input(shape=(28,28,1))\n",
    "conv1 = Conv2D(32, kernel_size=3, activation='relu')(visible)\n",
    "pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "conv2 = Conv2D(32, kernel_size=3, activation='relu')(pool1)\n",
    "pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "flat = Flatten()(pool2)\n",
    "hidden1 = Dense(100, activation='relu')(flat)\n",
    "output = Dense(10, activation='softmax')(hidden1)\n",
    "model3 = Model(inputs=visible, outputs=output)\n",
    "# summarize layers\n",
    "print(model3.summary())\n",
    "# plot graph\n",
    "plot_model(model3, to_file='convolutional_neural_network.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = 128\n",
    "E = 10\n",
    "model3.compile(loss='categorical_crossentropy', optimizer='rmsprop', \n",
    "              metrics=['accuracy'])\n",
    "log = model3.fit(X_train, Y_train, batch_size=B, epochs=E,\n",
    "                 verbose=1, validation_data=(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.plot(log.history['loss'], label='Training')\n",
    "pl.plot(log.history['val_loss'], label='Testing')\n",
    "pl.legend()\n",
    "pl.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_test, metric_test = model3.evaluate(X_test, Y_test)\n",
    "print('Test loss:', loss_test)\n",
    "print('Test accuracy:', metric_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional neural network - CNN with multiple path and shared input layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shared Input Layer\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras.layers import Flatten, Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D, concatenate\n",
    "\n",
    "# input layer\n",
    "visible = Input(shape=(28,28,1))\n",
    "# first feature extractor\n",
    "conv1 = Conv2D(32, kernel_size=3, activation='relu')(visible)\n",
    "drop1 = Dropout(0.2)(conv1)\n",
    "pool1 = MaxPooling2D(pool_size=(2, 2))(drop1)\n",
    "flat1 = Flatten()(pool1)\n",
    "# second feature extractor\n",
    "conv2 = Conv2D(32, kernel_size=6, activation='relu')(visible)\n",
    "drop2 = Dropout(0.2)(conv2)\n",
    "pool2 = MaxPooling2D(pool_size=(2, 2))(drop2)\n",
    "flat2 = Flatten()(pool2)\n",
    "# merge feature extractors\n",
    "merge = concatenate([flat1, flat2])\n",
    "# interpretation layer\n",
    "hidden1 = Dense(100, activation='relu')(merge)\n",
    "# prediction output\n",
    "output = Dense(10, activation='softmax')(hidden1)\n",
    "model4 = Model(inputs=visible, outputs=output)\n",
    "# summarize layers\n",
    "print(model4.summary())\n",
    "# plot graph\n",
    "plot_model(model4, to_file='shared_input_layer.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = 128\n",
    "E = 10\n",
    "model4.compile(loss='categorical_crossentropy', optimizer='rmsprop', \n",
    "              metrics=['accuracy'])\n",
    "log = model4.fit(X_train, Y_train, batch_size=B, epochs=E,\n",
    "                 verbose=1, validation_data=(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_test, metric_test = model4.evaluate(X_test, Y_test)\n",
    "print('Test loss:', loss_test)\n",
    "print('Test accuracy:', metric_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.plot(log.history['accuracy'], label='Training')\n",
    "pl.plot(log.history['val_accuracy'], label='Testing')\n",
    "pl.legend()\n",
    "pl.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conv neural network - CNN with multiple path, multiple features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shared Input Layer\n",
    "\n",
    "# input layer\n",
    "visible = Input(shape=(28,28,1))\n",
    "# first feature extractor\n",
    "conv1 = Conv2D(32, kernel_size=3, activation='relu')(visible)\n",
    "drop1 = Dropout(0.2)(conv1)\n",
    "pool1 = MaxPooling2D(pool_size=(2, 2))(drop1)\n",
    "flat1 = Flatten()(pool1)\n",
    "# second feature extractor\n",
    "conv2 = Conv2D(32, kernel_size=3, activation='relu')(pool1)\n",
    "drop2 = Dropout(0.2)(conv2)\n",
    "pool2 = MaxPooling2D(pool_size=(2, 2))(drop2)\n",
    "flat2 = Flatten()(pool2)\n",
    "# third feature extractor\n",
    "conv3 = Conv2D(32, kernel_size=3, activation='relu')(pool2)\n",
    "drop3 = Dropout(0.2)(conv3)\n",
    "pool3 = MaxPooling2D(pool_size=(2, 2))(drop3)\n",
    "flat3 = Flatten()(pool3)\n",
    "# merge feature extractors\n",
    "merge = concatenate([flat1, flat2, flat3])\n",
    "# interpretation layer\n",
    "hidden1 = Dense(100, activation='relu')(merge)\n",
    "# prediction output\n",
    "output = Dense(10, activation='softmax')(hidden1)\n",
    "model5 = Model(inputs=visible, outputs=output)\n",
    "# summarize layers\n",
    "print(model5.summary())\n",
    "# plot graph\n",
    "plot_model(model5, to_file='shared_input_layer_multi_feat.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "B = 128\n",
    "E = 10\n",
    "checkpoint = ModelCheckpoint('model-{epoch:03d}.h5', verbose=1, \n",
    "                             monitor='val_accuracy',save_best_only=True, \n",
    "                             mode='auto')\n",
    "model5.compile(loss='categorical_crossentropy', optimizer='rmsprop', \n",
    "              metrics=['accuracy'])\n",
    "log = model5.fit(X_train, Y_train, batch_size=B, epochs=E,\n",
    "                 verbose=1, validation_data=(X_test, Y_test), \n",
    "                 callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model5.load_weights(filepath = 'model-010.h5')\n",
    "loss_test, metric_test = model5.evaluate(X_test, Y_test)\n",
    "print('Test loss:', loss_test)\n",
    "print('Test accuracy:', metric_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.plot(log.history['accuracy'], label='Training')\n",
    "pl.plot(log.history['val_accuracy'], label='Testing')\n",
    "pl.legend()\n",
    "pl.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}