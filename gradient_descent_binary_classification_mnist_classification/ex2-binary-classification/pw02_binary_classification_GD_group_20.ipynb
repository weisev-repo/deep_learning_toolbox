{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Group 20 members:\n",
    "- Janick Michot:          michojan@students.zhaw.ch\n",
    "- Andre von Aarburg :     andre.vonaarbrug@ost.ch\n",
    "- Manuel Weiss:           weissman@students.zhaw.ch"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "98670d0a-08fb-4645-887b-4eb5d02cff00",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Binary Classification\n",
    "\n",
    "Here, we use a tabular dataset from kaggle (https://www.kaggle.com/sammy123/lower-back-pain-symptoms-dataset) with features on patients physical spine details possibly suited for classifying whether the person is 'abnormal' or 'normal' - possibly suffers back pain or not.   \n",
    "\n",
    "We here just want to see how the training works with logistic regression (binary case). We set aside a proper handling of the learning experiment by splitting the data into a train and test partition (in general we would even have a validation partition). We focus here on making the system learn something. \n",
    "\n",
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "275b5e04-7dbc-461d-9e63-c4e1c3852c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df shape:  (310, 13)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Col1</th>\n",
       "      <th>Col2</th>\n",
       "      <th>Col3</th>\n",
       "      <th>Col4</th>\n",
       "      <th>Col5</th>\n",
       "      <th>Col6</th>\n",
       "      <th>Col7</th>\n",
       "      <th>Col8</th>\n",
       "      <th>Col9</th>\n",
       "      <th>Col10</th>\n",
       "      <th>Col11</th>\n",
       "      <th>Col12</th>\n",
       "      <th>Class_att</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63.027817</td>\n",
       "      <td>22.552586</td>\n",
       "      <td>39.609117</td>\n",
       "      <td>40.475232</td>\n",
       "      <td>98.672917</td>\n",
       "      <td>-0.254400</td>\n",
       "      <td>0.744503</td>\n",
       "      <td>12.5661</td>\n",
       "      <td>14.5386</td>\n",
       "      <td>15.30468</td>\n",
       "      <td>-28.658501</td>\n",
       "      <td>43.5123</td>\n",
       "      <td>Abnormal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>39.056951</td>\n",
       "      <td>10.060991</td>\n",
       "      <td>25.015378</td>\n",
       "      <td>28.995960</td>\n",
       "      <td>114.405425</td>\n",
       "      <td>4.564259</td>\n",
       "      <td>0.415186</td>\n",
       "      <td>12.8874</td>\n",
       "      <td>17.5323</td>\n",
       "      <td>16.78486</td>\n",
       "      <td>-25.530607</td>\n",
       "      <td>16.1102</td>\n",
       "      <td>Abnormal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>68.832021</td>\n",
       "      <td>22.218482</td>\n",
       "      <td>50.092194</td>\n",
       "      <td>46.613539</td>\n",
       "      <td>105.985135</td>\n",
       "      <td>-3.530317</td>\n",
       "      <td>0.474889</td>\n",
       "      <td>26.8343</td>\n",
       "      <td>17.4861</td>\n",
       "      <td>16.65897</td>\n",
       "      <td>-29.031888</td>\n",
       "      <td>19.2221</td>\n",
       "      <td>Abnormal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>69.297008</td>\n",
       "      <td>24.652878</td>\n",
       "      <td>44.311238</td>\n",
       "      <td>44.644130</td>\n",
       "      <td>101.868495</td>\n",
       "      <td>11.211523</td>\n",
       "      <td>0.369345</td>\n",
       "      <td>23.5603</td>\n",
       "      <td>12.7074</td>\n",
       "      <td>11.42447</td>\n",
       "      <td>-30.470246</td>\n",
       "      <td>18.8329</td>\n",
       "      <td>Abnormal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>49.712859</td>\n",
       "      <td>9.652075</td>\n",
       "      <td>28.317406</td>\n",
       "      <td>40.060784</td>\n",
       "      <td>108.168725</td>\n",
       "      <td>7.918501</td>\n",
       "      <td>0.543360</td>\n",
       "      <td>35.4940</td>\n",
       "      <td>15.9546</td>\n",
       "      <td>8.87237</td>\n",
       "      <td>-16.378376</td>\n",
       "      <td>24.9171</td>\n",
       "      <td>Abnormal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Col1       Col2       Col3       Col4        Col5       Col6  \\\n",
       "0  63.027817  22.552586  39.609117  40.475232   98.672917  -0.254400   \n",
       "1  39.056951  10.060991  25.015378  28.995960  114.405425   4.564259   \n",
       "2  68.832021  22.218482  50.092194  46.613539  105.985135  -3.530317   \n",
       "3  69.297008  24.652878  44.311238  44.644130  101.868495  11.211523   \n",
       "4  49.712859   9.652075  28.317406  40.060784  108.168725   7.918501   \n",
       "\n",
       "       Col7     Col8     Col9     Col10      Col11    Col12 Class_att  \n",
       "0  0.744503  12.5661  14.5386  15.30468 -28.658501  43.5123  Abnormal  \n",
       "1  0.415186  12.8874  17.5323  16.78486 -25.530607  16.1102  Abnormal  \n",
       "2  0.474889  26.8343  17.4861  16.65897 -29.031888  19.2221  Abnormal  \n",
       "3  0.369345  23.5603  12.7074  11.42447 -30.470246  18.8329  Abnormal  \n",
       "4  0.543360  35.4940  15.9546   8.87237 -16.378376  24.9171  Abnormal  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"./Dataset_spine.csv\") # possibly modify!\n",
    "df = df.drop(columns=['Unnamed: 13'])\n",
    "N  = df.shape[0]\n",
    "print(\"df shape: \", df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cb51abff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5000)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "edfb0957-0a55-4437-9076-dd1cad0354c0",
   "metadata": {},
   "source": [
    "### Normalization and Turning into Torch Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "418ab33a-b905-4b9c-aff9-aa0e48bf9a6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1468,  0.5006, -0.6641, -0.1847, -1.4453, -0.7069,  0.9501, -1.0134,\n",
      "          0.4336,  1.1652, -1.1947,  1.7096],\n",
      "        [-1.2439, -0.7476, -1.4507, -1.0398, -0.2640, -0.5786, -0.2022, -0.9762,\n",
      "          1.3142,  1.6768, -0.9388, -0.9125],\n",
      "        [ 0.4836,  0.4672, -0.0991,  0.2726, -0.8962, -0.7941,  0.0067,  0.6381,\n",
      "          1.3006,  1.6333, -1.2252, -0.6147],\n",
      "        [ 0.5106,  0.7104, -0.4107,  0.1259, -1.2054, -0.4016, -0.3626,  0.2591,\n",
      "         -0.1050, -0.1759, -1.3428, -0.6519],\n",
      "        [-0.6256, -0.7884, -1.2727, -0.2155, -0.7323, -0.4893,  0.2463,  1.6404,\n",
      "          0.8501, -1.0580, -0.1902, -0.0697]], dtype=torch.float64)\n",
      "tensor([[1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1]], dtype=torch.int32)\n",
      "torch.Size([310, 12]) torch.Size([310, 1])\n"
     ]
    }
   ],
   "source": [
    "x0 = torch.from_numpy(df.values[:,0:-1].astype(np.float64))\n",
    "X = (x0-torch.mean(x0, dim=0))/torch.std(x0,dim=0)\n",
    "Y = torch.tensor(('Abnormal'==df.values[:,-1])).int().reshape(-1,1)\n",
    "\n",
    "print(X[:5, :])\n",
    "print(Y[:5, :])\n",
    "print(X.shape, Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf95eefb-3767-4884-a799-7617e9428a5c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### (Binary) Logistic Regression\n",
    "\n",
    "Data:  $\\,\\qquad X = \\left(\\begin{array}{cccc} 1 & X_{11} & \\dots & X_{1n} \\\\ \\vdots & \\vdots & \\vdots & \\vdots \\\\ 1 & X_{N1} & \\dots & X_{Nn}\\end{array}\\right)\\qquad$ and $\\qquad Y = \\left(\\begin{array}{c} Y_{1} \\\\ \\vdots \\\\ Y_{N} \\end{array}\\right)$\n",
    "\n",
    "Model: $\\qquad\\hat{Y}(X;W) = \\sigma\\left(X W^\\intercal\\right) \\qquad$ where $\\qquad W = \\left(\\begin{array}{c} W_0 \\\\ W_1 \\\\ \\vdots \\\\ W_n \\end{array}\\right)$\n",
    "\n",
    "The model outputs the probability of observing in a sample $x$ a '1' (Abnormal).\n",
    "\n",
    "Cost:  $\\,\\qquad C(W) = -\\frac{1}{N}\\sum_j \\left(Y_j\\log(\\hat{Y}_j(X;W)) + (1-Y_j)\\log(1-\\hat{Y}_j(X;W))\\right)$\n",
    "\n",
    "__Remark:__ Note that the logarithm diverges at arguments approaching 0. Make sure that you don't run into numerical issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "79210499-8d67-4f08-9da3-91a9b2f93f89",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.0000,  0.1468,  0.5006, -0.6641, -0.1847, -1.4453, -0.7069,  0.9501,\n",
      "         -1.0134,  0.4336,  1.1652, -1.1947,  1.7096],\n",
      "        [ 1.0000, -1.2439, -0.7476, -1.4507, -1.0398, -0.2640, -0.5786, -0.2022,\n",
      "         -0.9762,  1.3142,  1.6768, -0.9388, -0.9125],\n",
      "        [ 1.0000,  0.4836,  0.4672, -0.0991,  0.2726, -0.8962, -0.7941,  0.0067,\n",
      "          0.6381,  1.3006,  1.6333, -1.2252, -0.6147],\n",
      "        [ 1.0000,  0.5106,  0.7104, -0.4107,  0.1259, -1.2054, -0.4016, -0.3626,\n",
      "          0.2591, -0.1050, -0.1759, -1.3428, -0.6519],\n",
      "        [ 1.0000, -0.6256, -0.7884, -1.2727, -0.2155, -0.7323, -0.4893,  0.2463,\n",
      "          1.6404,  0.8501, -1.0580, -0.1902, -0.0697]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# compose torch tensors X of shape (N,13) by inserting a column with 1's as first column  \n",
    "X = torch.cat((torch.ones(N,1),X), dim=1)\n",
    "print(X[:5,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5053b3b8-4e77-45c7-8a57-e9abe98c0910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement methods for predicting the probability of having label 0 or 1 (W with shape (1,13))\n",
    "def predict(X,W):\n",
    "    # YOUR CODE (START)\n",
    "    return torch.sigmoid(X@W.T)\n",
    "    # YOUR CODE (END)\n",
    "\n",
    "def cost(X,Y,W):\n",
    "    # YOUR CODE (START)\n",
    "    Yhat = predict(X, W)\n",
    "    return -(1/N)*(torch.sum(torch.log(Yhat[Y==1]))+torch.sum(torch.log(1-Yhat[Y==0])))\n",
    "    # YOUR CODE (END)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "b73745b8-c2f5-4150-a8f7-2e49f72f3566",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def gradient_cost(X,Y,W):\n",
    "    # YOUR CODE (START)\n",
    "    Yhat = predict(X, W)\n",
    "    return -(Y-Yhat).T@X/N\n",
    "    # YOUR CODE (END)\n",
    "    \n",
    "def accuracy(Y,Yhat):\n",
    "    # YOUR CODE (START)\n",
    "    return ((sum((Y==torch.round(Yhat)).int()))/N).item()\n",
    "    # YOUR CODE (END)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da79340e-41e2-4b8a-a1c3-f94d0e00c69e",
   "metadata": {},
   "source": [
    "Just for testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "8dd82c26-6db7-43ec-84c5-2e1a9c998d08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0989], dtype=torch.float64)\n",
      "tensor(1.3321, dtype=torch.float64)\n",
      "tensor([[-3.1143e-01, -1.8297e-03,  1.7836e-04,  2.8013e-02, -2.4824e-03,\n",
      "          2.0895e-01, -3.8303e-02,  9.4155e-02,  1.9426e-02,  2.1341e-01,\n",
      "         -5.8543e-02,  4.2729e-02, -1.2563e-02]], dtype=torch.float64)\n",
      "0.5451613068580627\n"
     ]
    }
   ],
   "source": [
    "W = torch.randn((1,13), dtype=torch.double)\n",
    "print(predict(X[0],W))\n",
    "print(cost(X,Y,W))\n",
    "print(gradient_cost(X,Y,W))\n",
    "print(accuracy(Y,predict(X,W)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc000103-2e8d-4570-a9a6-c4aa062928cb",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "76f0b32a-fd22-4a94-b923-8714538a0204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy (max,end): 0.874194, 0.854839\n",
      "Training Cost (end): 0.284133\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# adjust if needed\n",
    "nepochs = 1000\n",
    "lr = 1.0\n",
    "\n",
    "## initial parameter\n",
    "W = torch.randn((1,13), dtype=torch.double)\n",
    "\n",
    "# track the costs\n",
    "costs = [cost(X,Y,W)]\n",
    "accs = [accuracy(Y,predict(X,W))]\n",
    "\n",
    "# YOUR CODE (START)\n",
    "\n",
    "# loop over the epochs: update parameter values, compute the cost and add it to the costs list\n",
    "for epoch in range(nepochs):\n",
    "    W = W - lr * gradient_cost(X, Y, W)\n",
    "    costs.append(cost(X,Y,W))\n",
    "    accs.append(accuracy(Y,predict(X,W)))\n",
    "\n",
    "# YOUR CODE (END)\n",
    "    \n",
    "# some output\n",
    "accs = np.array(accs)\n",
    "\n",
    "print(\"Training Accuracy (max,end): %f, %f\"%(np.max(accs), accs[-1]))\n",
    "print(\"Training Cost (end): %f\"%costs[-1].item())\n",
    "plt.figure(1)\n",
    "plt.plot(range(nepochs+1),costs)\n",
    "plt.figure(2)\n",
    "plt.plot(range(nepochs+1),accs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fc7cf8-ffbd-4d78-a015-7cf2c540ad4c",
   "metadata": {},
   "source": [
    "### Different Learning Rates\n",
    "\n",
    "Play with different learning rates: Explore for what learning rates \n",
    "- the learning is most efficient\n",
    "- the learning yet works\n",
    "- the learning does not work anymore (learning rate too large)\n",
    "\n",
    "Explain the different scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "e37238ff-7326-465e-8139-b12bcb5e9cdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lr</th>\n",
       "      <th>count</th>\n",
       "      <th>training_cost_end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.645354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.3</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0.461747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.5</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.459822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.7</td>\n",
       "      <td>1933.0</td>\n",
       "      <td>0.284112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.9</td>\n",
       "      <td>1590.0</td>\n",
       "      <td>0.284111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.1</td>\n",
       "      <td>1358.0</td>\n",
       "      <td>0.284110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.3</td>\n",
       "      <td>1189.0</td>\n",
       "      <td>0.284110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.5</td>\n",
       "      <td>1060.0</td>\n",
       "      <td>0.284110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.7</td>\n",
       "      <td>958.0</td>\n",
       "      <td>0.284109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.9</td>\n",
       "      <td>875.0</td>\n",
       "      <td>0.284109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2.1</td>\n",
       "      <td>807.0</td>\n",
       "      <td>0.284109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2.3</td>\n",
       "      <td>749.0</td>\n",
       "      <td>0.284109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2.5</td>\n",
       "      <td>699.0</td>\n",
       "      <td>0.284109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2.7</td>\n",
       "      <td>656.0</td>\n",
       "      <td>0.284109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2.9</td>\n",
       "      <td>618.0</td>\n",
       "      <td>0.284109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>3.1</td>\n",
       "      <td>585.0</td>\n",
       "      <td>0.284109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>3.3</td>\n",
       "      <td>555.0</td>\n",
       "      <td>0.284109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>3.5</td>\n",
       "      <td>529.0</td>\n",
       "      <td>0.284109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>3.7</td>\n",
       "      <td>505.0</td>\n",
       "      <td>0.284109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>3.9</td>\n",
       "      <td>483.0</td>\n",
       "      <td>0.284109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>4.1</td>\n",
       "      <td>463.0</td>\n",
       "      <td>0.284109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>4.3</td>\n",
       "      <td>445.0</td>\n",
       "      <td>0.284109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>4.5</td>\n",
       "      <td>428.0</td>\n",
       "      <td>0.284109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>4.7</td>\n",
       "      <td>413.0</td>\n",
       "      <td>0.284109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>4.9</td>\n",
       "      <td>398.0</td>\n",
       "      <td>0.284109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>5.1</td>\n",
       "      <td>385.0</td>\n",
       "      <td>0.284109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>5.3</td>\n",
       "      <td>373.0</td>\n",
       "      <td>0.284109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>5.5</td>\n",
       "      <td>361.0</td>\n",
       "      <td>0.284109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>5.7</td>\n",
       "      <td>349.0</td>\n",
       "      <td>0.284109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>5.9</td>\n",
       "      <td>337.0</td>\n",
       "      <td>0.284109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>6.1</td>\n",
       "      <td>326.0</td>\n",
       "      <td>0.284109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>6.3</td>\n",
       "      <td>313.0</td>\n",
       "      <td>0.284109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>6.5</td>\n",
       "      <td>300.0</td>\n",
       "      <td>0.284109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>6.7</td>\n",
       "      <td>138.0</td>\n",
       "      <td>0.284123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>6.9</td>\n",
       "      <td>296.0</td>\n",
       "      <td>0.284109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>7.1</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>0.286594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>7.3</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>0.294773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>7.5</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>0.302800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>7.7</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>0.310725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>7.9</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>0.318572</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     lr    count  training_cost_end\n",
       "0   0.1     20.0           0.645354\n",
       "1   0.3     33.0           0.461747\n",
       "2   0.5     20.0           0.459822\n",
       "3   0.7   1933.0           0.284112\n",
       "4   0.9   1590.0           0.284111\n",
       "5   1.1   1358.0           0.284110\n",
       "6   1.3   1189.0           0.284110\n",
       "7   1.5   1060.0           0.284110\n",
       "8   1.7    958.0           0.284109\n",
       "9   1.9    875.0           0.284109\n",
       "10  2.1    807.0           0.284109\n",
       "11  2.3    749.0           0.284109\n",
       "12  2.5    699.0           0.284109\n",
       "13  2.7    656.0           0.284109\n",
       "14  2.9    618.0           0.284109\n",
       "15  3.1    585.0           0.284109\n",
       "16  3.3    555.0           0.284109\n",
       "17  3.5    529.0           0.284109\n",
       "18  3.7    505.0           0.284109\n",
       "19  3.9    483.0           0.284109\n",
       "20  4.1    463.0           0.284109\n",
       "21  4.3    445.0           0.284109\n",
       "22  4.5    428.0           0.284109\n",
       "23  4.7    413.0           0.284109\n",
       "24  4.9    398.0           0.284109\n",
       "25  5.1    385.0           0.284109\n",
       "26  5.3    373.0           0.284109\n",
       "27  5.5    361.0           0.284109\n",
       "28  5.7    349.0           0.284109\n",
       "29  5.9    337.0           0.284109\n",
       "30  6.1    326.0           0.284109\n",
       "31  6.3    313.0           0.284109\n",
       "32  6.5    300.0           0.284109\n",
       "33  6.7    138.0           0.284123\n",
       "34  6.9    296.0           0.284109\n",
       "35  7.1  10000.0           0.286594\n",
       "36  7.3  10000.0           0.294773\n",
       "37  7.5  10000.0           0.302800\n",
       "38  7.7  10000.0           0.310725\n",
       "39  7.9  10000.0           0.318572"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "decent_limit = 1 * 10**-4\n",
    "nepochs = 10000\n",
    "initial_lr=0.1\n",
    "lr_step=0.2\n",
    "n_step = 40\n",
    "\n",
    "initial_W = torch.randn((1, 13), dtype=torch.double)\n",
    "\n",
    "def efficiency_test(initial_W, initial_lr, lr_step, decent_limit):\n",
    "    lr = initial_lr\n",
    "    count = 0\n",
    "    counts = pd.DataFrame(columns=['lr', 'count', 'training_cost_end'])\n",
    "    for i in range(n_step):\n",
    "        costs = [cost(X,Y,initial_W)]\n",
    "        count = 0\n",
    "        W = initial_W\n",
    "        weight_adjustment_sum = 5\n",
    "        while abs(weight_adjustment_sum) > decent_limit and count < nepochs:\n",
    "            gradient = gradient_cost(X, Y, W)\n",
    "            weight_adjustment = lr * gradient\n",
    "            W = W - weight_adjustment\n",
    "            weight_adjustment_sum = torch.sum(weight_adjustment).item()\n",
    "            costs.append(cost(X, Y, W))\n",
    "            count += 1\n",
    "        counts.loc[len(counts.index)] = [lr, count, costs[-1].item()]\n",
    "        counts.reset_index(drop=True)\n",
    "        lr += lr_step\n",
    "    counts.sort_values(by='count', ascending=False)\n",
    "    display(counts)\n",
    "    \n",
    "\n",
    "efficiency_test(initial_W, initial_lr, lr_step, decent_limit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667ccec1",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "In order to play with the learning rates we wrote the above simulation.\n",
    "The displayed table shows the result of different learning rates and the count of how many iterations it took until\n",
    "to reach no further descent (adjustable by the decent limit).\n",
    "\n",
    "Play with different learning rates: Explore for what learning rates\n",
    "- the learning is most efficient\n",
    "    - Most of the cases the learning rate was most efficient arund 6 or 6.8\n",
    "- the learning yet works\n",
    "    - The interessting thing is that it always collapse by a lr higher 6.9\n",
    "- the learning does not work anymore (learning rate too large)\n",
    "    - With a learning rate of 7 it doesn't work anymore to find a local minimum\n",
    "    \n",
    "Also interessting to see was that it not always find the global minimum rather a local one. If we ran the simulation multiple times it sometimes found the global minimum with a small learning rate (0.1 -0.5) and sometimes (like the one above) not. So it seems that small lr are more susceptible to fail the global minimum than larger ones (or at least in this case). "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}